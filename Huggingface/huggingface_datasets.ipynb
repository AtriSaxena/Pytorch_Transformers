{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets in HuggingFace\n",
    "In this we will study how we can Load Dataset, extract details, apply tokenization, split and upload back to hugging face.\n",
    "- We will load data, create mini dataset and upload back to huggingface. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Dataset: \n",
    "#### `load_dataset_builder`: To get details of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb718776ab74ffb8ed7b8d4d620fd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Programs\\anaconda\\envs\\gpu_env2\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Atri Saxena\\.cache\\huggingface\\hub\\datasets--cfilt--iitb-english-hindi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6622c7cef31747cd8ec0c224f9927887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "dataset_name = \"cfilt/iitb-english-hindi\"\n",
    "\n",
    "ds_builder = load_dataset_builder(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': Value(dtype='string', id=None),\n",
       "  'hi': Value(dtype='string', id=None)}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If youâ€™re happy with the dataset, then load it with `load_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data: 1659083\n",
      "One object of dataset: {'translation': {'en': 'Give your application an accessibility workout', 'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of data: {len(dataset)}\")\n",
    "print(f\"One object of dataset: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object of dataset: [{'translation': {'en': 'exotic', 'hi': 'à¤…à¤¨à¤­à¥‹'}}]\n"
     ]
    }
   ],
   "source": [
    "# view random data \n",
    "import random \n",
    "\n",
    "print(f\"object of dataset: {random.choices(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': [{'en': 'Highlight duration', 'hi': 'à¤…à¤µà¤§à¤¿ à¤•à¥‹ à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤°à¤•à¥‡à¤‚'},\n",
       "  {'en': 'The duration of the highlight box when selecting accessible nodes',\n",
       "   'hi': 'à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯ à¤†à¤¸à¤‚à¤§à¤¿ (à¤¨à¥‹à¤¡) à¤•à¥‹ à¤šà¥à¤¨à¤¤à¥‡ à¤¸à¤®à¤¯ à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤¬à¤•à¥à¤¸à¥‡ à¤•à¥€ à¤…à¤µà¤§à¤¿'},\n",
       "  {'en': 'Highlight border color',\n",
       "   'hi': 'à¤¸à¥€à¤®à¤¾à¤‚à¤¤ (à¤¬à¥‹à¤°à¥à¤¡à¤°) à¤•à¥‡ à¤°à¤‚à¤— à¤•à¥‹ à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤•à¤°à¥‡à¤‚'},\n",
       "  {'en': 'The color and opacity of the highlight border.',\n",
       "   'hi': 'à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤¸à¥€à¤®à¤¾à¤‚à¤¤ à¤•à¤¾ à¤°à¤‚à¤— à¤”à¤° à¤…à¤ªà¤¾à¤°à¤¦à¤°à¥à¤¶à¤¿à¤¤à¤¾à¥¤ '},\n",
       "  {'en': 'Highlight fill color', 'hi': 'à¤­à¤°à¤¾à¤ˆ à¤•à¥‡ à¤°à¤‚à¤— à¤•à¥‹ à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤•à¤°à¥‡à¤‚'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Give your application an accessibility workout', 'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'}}\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "for example in iterable_dataset:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create an `IterableDataset` from an existing `Dataset`, but it is faster than streaming mode because the dataset is streamed from local files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Give your application an accessibility workout', 'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'}}\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = dataset.to_iterable_dataset()\n",
    "for example in iterable_dataset:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An IterableDataset progressively iterates over a dataset one example at a time, so you donâ€™t have to wait for the whole dataset to download before you can use it. As you can imagine, this is quite useful for large datasets you want to use immediately!\n",
    "\n",
    "However, this means an IterableDatasetâ€™s behavior is different from a regular Dataset. You donâ€™t get random access to examples in an IterableDataset. Instead, you should iterate over its elements, for example, by calling next(iter()) or with a for loop to return the next item from the IterableDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Give your application an accessibility workout',\n",
       "  'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(iterable_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can return a subset of the dataset with a specific number of examples in it with IterableDataset.take():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation': {'en': 'Give your application an accessibility workout',\n",
       "   'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'}},\n",
       " {'translation': {'en': 'Accerciser Accessibility Explorer',\n",
       "   'hi': 'à¤à¤•à¥à¤¸à¥‡à¤°à¥à¤¸à¤¾à¤‡à¤¸à¤° à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤…à¤¨à¥à¤µà¥‡à¤·à¤•'}},\n",
       " {'translation': {'en': 'The default plugin layout for the bottom panel',\n",
       "   'hi': 'à¤¨à¤¿à¤šà¤²à¥‡ à¤ªà¤Ÿà¤² à¤•à¥‡ à¤²à¤¿à¤ à¤¡à¤¿à¤«à¥‹à¤²à¥à¤Ÿ à¤ªà¥à¤²à¤—-à¤‡à¤¨ à¤–à¤¾à¤•à¤¾'}}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iterable_dataset.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ðŸ¤— Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, youâ€™ll need to:\n",
    "\n",
    "Tokenize a text dataset.\n",
    "Resample an audio dataset.\n",
    "Apply transforms to an image dataset.\n",
    "\n",
    "The last preprocessing step is usually setting your dataset format to be compatible with your machine learning frameworkâ€™s expected input format.\n",
    "\n",
    "In this tutorial, youâ€™ll also need to install the ðŸ¤— Transformers library:\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "Grab a dataset of your choice and follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text\n",
    "Models cannot process raw text, so youâ€™ll need to convert the text into numbers. Tokenization provides a way to do this by dividing text into individual words called tokens. Tokens are finally converted to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c2cbeab1444fabada02ccbf3582143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Programs\\anaconda\\envs\\gpu_env2\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Atri Saxena\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fb8023d65c48a0b7e1d11fb73d8bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dc41d4286148ce8781b9ff42880c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6faa192e9cd4ec3bdc66ec746ba745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Call your tokenizer on the first row of text in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:Give your application an accessibility workout\n",
      "Tokens:{'input_ids': [101, 2507, 2115, 4646, 2019, 23661, 27090, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence:{dataset[0]['translation']['en']}\")\n",
    "print(f\"Tokens:{tokenizer(dataset[0]['translation']['en'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚\n",
      "Tokens:{'input_ids': [101, 1311, 29864, 29863, 1311, 29863, 29864, 29869, 29868, 29879, 29853, 1315, 29879, 1328, 29875, 29854, 29863, 29878, 29868, 29859, 29876, 1335, 29868, 29876, 29868, 29876, 29867, 1315, 29876, 1334, 29876, 29866, 1325, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence:{dataset[0]['translation']['hi']}\")\n",
    "print(f\"Tokens:{tokenizer(dataset[0]['translation']['hi'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary with three items:\n",
    "\n",
    "- **input_ids:** the numbers representing the tokens in the text.\n",
    "- **token_type_ids:** indicates which sequence a token belongs to if there is more than one sequence.\n",
    "- **attention_mask:** indicates whether a token should be masked or not.\n",
    "These values are actually the model inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The fastest way to tokenize your entire dataset is to use the map() function. This function speeds up tokenization by applying the tokenizer to batches of examples instead of individual examples. Set the batched parameter to True:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Before Tokenization create a MINI Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_samples = 20000  # Number of random samples\n",
    "random_indices = random.sample(range(len(dataset)), num_samples)\n",
    "mini_dataset = dataset.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Location at (date) \\\\t', 'hi': 'à¤¸à¥à¤¥à¤¾à¤¨ (à¤¤à¤¿à¤¥à¤¿) à¤ªà¤°\\\\t'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_english(example):\n",
    "    return tokenizer(example['translation']['en'], max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_hindi(example):\n",
    "    return tokenizer(example['translation']['hi'], max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2294273d21d04710a73c8b8f8bed4ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini_dataset_tokens_en = mini_dataset.map(tokenization_english, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916b99c45404c5ba2d0810837eff7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini_dataset_tokens_hi = mini_dataset.map(tokenization_hindi, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set the format of your dataset to be compatible with your machine learning framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d1997feeb443dba9d6ef9cad013e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini_dataset_tokens_en.save_to_disk('tokenizer_en.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset_tokens_en.set_format(type=\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3295, 2012, 1006, 3058, 1007, 1032, 1056,  102])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset_tokens_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0089045fffc241b58a8ba8f05e4816b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5382716"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset.to_parquet(path_or_buf=\"B:\\CODE\\Pytorch_Transformers\\Huggingface\\\\train.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
