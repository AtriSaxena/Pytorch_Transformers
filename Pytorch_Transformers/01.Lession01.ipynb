{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Transformers\n",
    "- In this notebook we will cover Pytorch Transformers layer which is under `torch.nn`.\n",
    "    - `nn.Transformer` -> A transformer model.\n",
    "    - `nn.TransformerEncoder` -> TransformerEncoder is a stack of N encoder layers.\n",
    "    - `nn.TransformerDecoder` -> TransformerDecoder is a stack of N decoder layers.\n",
    "    - `nn.TransformerEncoderLayer` -> TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    - `nn.TransformerDecoderLayer` -> TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "\n",
    "- We will also try out various kind of problems with text:\n",
    "    - `Text Classification`\n",
    "    - `Text Generation`\n",
    "    - `Translation`\n",
    "    - `Next Word Prediction`\n",
    "    - `Text Summarization`\n",
    "    - `Question Answering (Q&A)`\n",
    "    - `Text-to-Speech`\n",
    "    - `Speech-to-Text`\n",
    "    - `Text Simialarity & Semantic search`\n",
    "    - `Sentence correction - Grammar and spellings.` \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding of different function and parameters.\n",
    "\n",
    "1. `nn.Transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter           | Description                                      | Example |\n",
    "|---------------------|--------------------------------------------------|---------|\n",
    "| `d_model`          | Embedding dimension of the model                 | `512`   |\n",
    "| `nhead`            | Number of attention heads                        | `8`     |\n",
    "| `num_encoder_layers` | Number of encoder layers                      | `6`     |\n",
    "| `num_decoder_layers` | Number of decoder layers                      | `6`     |\n",
    "| `dim_feedforward`  | Hidden layer size in feed-forward networks       | `2048`  |\n",
    "| `dropout`          | Dropout probability (to prevent overfitting)     | `0.1`   |\n",
    "| `batch_first`      | If `True`, input is in (batch, seq, feature) format | `True`  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Programs\\anaconda\\envs\\gpu_env2\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=8, num_encoder_layers=6, d_model=256, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand((1, 10, 512))\n",
    "tgt = torch.rand((1, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `nn.TransformerEncoder`\n",
    "\n",
    "TransformerEncoder is a stack of N encoder layers.\n",
    "\n",
    "**Parameters**\n",
    "- **encoder_layer (TransformerEncoderLayer) –** an instance of the TransformerEncoderLayer() class (required).\n",
    "\n",
    "- **num_layers (int) –** the number of sub-encoder-layers in the encoder (required).\n",
    "\n",
    "- **norm (Optional[Module]) –** the layer normalization component (optional).\n",
    "\n",
    "- **enable_nested_tensor (bool) –** if True, input will automatically convert to nested tensor (and convert back on output). This will improve the overall performance of TransformerEncoder when padding rate is high. Default: True (enabled).\n",
    "\n",
    "### 3. `nn.TransformerEncoderLayer`\n",
    "\n",
    "TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "- **`d_model` (int)** – The number of expected features in the input (**required**).\n",
    "- **`nhead` (int)** – The number of heads in the multi-head attention models (**required**).\n",
    "- **`dim_feedforward` (int, default=`2048`)** – The dimension of the feedforward network model.\n",
    "- **`dropout` (float, default=`0.1`)** – The dropout value.\n",
    "- **`activation` (Union[str, Callable[[Tensor], Tensor]], default=`\"relu\"`)** –  \n",
    "  The activation function of the intermediate layer. Can be a string (`\"relu\"` or `\"gelu\"`) or a callable function.\n",
    "- **`layer_norm_eps` (float, default=`1e-5`)** – The epsilon value used in layer normalization.\n",
    "- **`batch_first` (bool, default=`False`)** –  \n",
    "  If `True`, the input and output tensors are provided as `(batch, seq, feature)`.  \n",
    "  If `False`, the format is `(seq, batch, feature)`.\n",
    "- **`norm_first` (bool, default=`False`)** –  \n",
    "  If `True`, layer normalization is applied **before** attention and feed-forward layers.  \n",
    "  If `False`, it is applied **after**.\n",
    "- **`bias` (bool, default=`True`)** –  \n",
    "  If `False`, Linear and LayerNorm layers will not learn an additive bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "B:\\Programs\\anaconda\\envs\\gpu_env2\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `TransformerDecoder`\n",
    "\n",
    "TransformerDecoder is a stack of N decoder layers.\n",
    "\n",
    "| Parameter        | Type                      | Required | Description |\n",
    "|-----------------|--------------------------|----------|-------------|\n",
    "| `decoder_layer` | `TransformerDecoderLayer` | ✅ Yes   | An instance of the `TransformerDecoderLayer()` class. |\n",
    "| `num_layers`   | `int`                     | ✅ Yes   | The number of sub-decoder layers in the decoder. |\n",
    "| `norm`         | `Optional[Module]`        | ❌ No    | The layer normalization component. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `TransformerDecoderLayer`\n",
    "\n",
    "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "\n",
    "| Parameter        | Type                                      | Default  | Description |\n",
    "|-----------------|-----------------------------------------|----------|-------------|\n",
    "| `d_model`       | `int`                                   | **Required** | The number of expected features in the input. |\n",
    "| `nhead`         | `int`                                   | **Required** | The number of heads in the multi-head attention models. |\n",
    "| `dim_feedforward` | `int`                                 | `2048`   | The dimension of the feedforward network model. |\n",
    "| `dropout`       | `float`                                 | `0.1`    | The dropout value. |\n",
    "| `activation`    | `Union[str, Callable[[Tensor], Tensor]]` | `\"relu\"` | The activation function of the intermediate layer, can be `\"relu\"`, `\"gelu\"`, or a callable function. |\n",
    "| `layer_norm_eps` | `float`                                | `1e-5`   | The epsilon value in layer normalization components. |\n",
    "| `batch_first`   | `bool`                                  | `False`  | If `True`, input and output tensors are in `(batch, seq, feature)` format; otherwise, `(seq, batch, feature)`. |\n",
    "| `norm_first`    | `bool`                                  | `False`  | If `True`, layer normalization is applied before self-attention, multi-head attention, and feedforward operations; otherwise, it's applied after. |\n",
    "| `bias`         | `bool`                                   | `True`   | If `False`, Linear and LayerNorm layers will not learn an additive bias. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer_decoder(tgt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lessions we will implement some of the problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
